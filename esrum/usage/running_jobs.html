

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>4.4. Running jobs using Slurm &#8212; Esrum Cluster  documentation</title>
    <link rel="stylesheet" href="../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/playback.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/libgif.js"></script>
    <script src="../_static/bizstyle.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.5. Running batch jobs" href="batch_jobs.html" />
    <link rel="prev" title="4.3. Projects, data, and home folders" href="filesystem.html" />
    <script defer src="../_static/js/playback.js"></script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="batch_jobs.html" title="4.5. Running batch jobs"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="filesystem.html" title="4.3. Projects, data, and home folders"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Esrum Cluster  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U"><span class="section-number">4. </span>Using the cluster</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">4.4. Running jobs using Slurm</a><ul>
<li><a class="reference internal" href="#running-commands-using-slurm">4.4.1. Running commands using Slurm</a></li>
<li><a class="reference internal" href="#cancelling-jobs">4.4.2. Cancelling jobs</a></li>
<li><a class="reference internal" href="#running-an-interactive-shell">4.4.3. Running an interactive shell</a></li>
<li><a class="reference internal" href="#reserving-resources-for-your-jobs">4.4.4. Reserving resources for your jobs</a><ul>
<li><a class="reference internal" href="#best-practice-for-reserving-resources">4.4.4.1. Best practice for reserving resources</a></li>
<li><a class="reference internal" href="#reserving-the-gpu-node">4.4.4.2. Reserving the GPU node</a></li>
<li><a class="reference internal" href="#monitoring-gpu-utilization">4.4.4.3. Monitoring GPU utilization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">4.4.5. Troubleshooting</a><ul>
<li><a class="reference internal" href="#error-requested-node-configuration-is-not-available">4.4.5.1. Error: Requested node configuration is not available</a></li>
</ul>
</li>
<li><a class="reference internal" href="#additional-resources">4.4.6. Additional resources</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="filesystem.html"
                        title="previous chapter"><span class="section-number">4.3. </span>Projects, data, and home folders</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="batch_jobs.html"
                        title="next chapter"><span class="section-number">4.5. </span>Running batch jobs</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="running-jobs-using-slurm">
<span id="page-running"></span><h1><span class="section-number">4.4. </span>Running jobs using Slurm<a class="headerlink" href="#running-jobs-using-slurm" title="Permalink to this headline">¶</a></h1>
<p>In order to run jobs on the Esrum cluster, you must connect to the head
node and queue them using the <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a> job management system. <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a> takes
care of automatically queuing and distribute jobs on the compute and GPU
nodes when the required resources are available.</p>
<p>This section describes how to run basic jobs using the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command,
how to start an interactive shell on a compute node, and how to reserve
the resources needed for your tasks.</p>
<p>If you need to run a number of similar jobs in parallel, for example
genotyping a set of samples or mapping FASTQ files to a reference
genome, then the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command can be used to automatically queue
multiple jobs. See the <a class="reference internal" href="batch_jobs.html#page-batch-jobs"><span class="std std-ref">Running batch jobs</span></a> for more information.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Resource intensive jobs <em>must</em> be run using <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a>. Tasks running on
the head node <em>will</em> be terminated without prior warning, in order to
prevent any impact on other users of the cluster.</p>
</div>
<div class="section" id="running-commands-using-slurm">
<h2><span class="section-number">4.4.1. </span>Running commands using Slurm<a class="headerlink" href="#running-commands-using-slurm" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> command is used to queue and execute commands on the
compute nodes, and for most part it should feel no different than
running a command without <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a>. Simply prefix your command with
<code class="docutils literal notranslate"><span class="pre">srun</span></code> and the queuing system takes care of running it on the first
available compute node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun gzip chr20.fasta
</pre></div>
</div>
<img alt="../_images/srun_minimal.gif" class="gif" src="../_images/srun_minimal.gif" />
<p>Except for the <code class="docutils literal notranslate"><span class="pre">srun</span></code> prefix, this is exactly as if you ran the
<code class="docutils literal notranslate"><span class="pre">gzip</span></code> command on the head node. However, if you need to pipe output
to a file or to another command, then you <em>must</em> wrap your commands in a
bash (or similar) script. The script can then be run using <code class="docutils literal notranslate"><span class="pre">srun</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun bash my_script.sh
</pre></div>
</div>
<img alt="../_images/srun_wrapped.gif" class="gif" src="../_images/srun_wrapped.gif" />
<p>By default task are allocated one CPU and 15 GB of RAM. If you need to
use additional resources, then see <a class="reference internal" href="#reserving-resources-for-your-jobs">Reserving resources for your jobs</a>
and <a class="reference internal" href="#reserving-the-gpu-node">Reserving the GPU node</a> below.</p>
</div>
<div class="section" id="cancelling-jobs">
<h2><span class="section-number">4.4.2. </span>Cancelling jobs<a class="headerlink" href="#cancelling-jobs" title="Permalink to this headline">¶</a></h2>
<p>To cancel a job running with srun, simply press <cite>Ctrl + c</cite> twice:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ srun gzip chr20.fasta
&lt;ctrl+c&gt; srun: interrupt <span class="o">(</span>one more within <span class="m">1</span> sec to abort<span class="o">)</span>
srun: <span class="nv">StepId</span><span class="o">=</span><span class="m">8717</span>.0 task <span class="m">0</span>: running
&lt;ctrl+c&gt; srun: sending Ctrl-C to <span class="nv">StepId</span><span class="o">=</span><span class="m">8717</span>.0
srun: Job step aborted: Waiting up to <span class="m">32</span> seconds <span class="k">for</span> job step to finish.
</pre></div>
</div>
<p>See also the <a class="reference internal" href="batch_jobs.html#managing-jobs"><span class="std std-ref">Cancelling jobs</span></a> section on the <a class="reference internal" href="batch_jobs.html#page-batch-jobs"><span class="std std-ref">Running batch jobs</span></a>
page.</p>
</div>
<div class="section" id="running-an-interactive-shell">
<h2><span class="section-number">4.4.3. </span>Running an interactive shell<a class="headerlink" href="#running-an-interactive-shell" title="Permalink to this headline">¶</a></h2>
<p>If you need to run an interactive process, for example if you need to
use an interactive R shell to process a large dataset, or if you just
need to experiment with running an computationally heavy process, then
you can start a shell on one of the compute nodes as follows:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[abc123@esrumhead01fl ~] $</span> srun --pty -- /bin/bash
<span class="gp">[abc123@esrumcmpn07fl ~] $</span>
</pre></div>
</div>
<p>Note that the hostname displayed changes from <code class="docutils literal notranslate"><span class="pre">esrumhead01fl</span></code> to
<code class="docutils literal notranslate"><span class="pre">esrumcmpn07fl</span></code>, where <code class="docutils literal notranslate"><span class="pre">esrumcmpn07fl</span></code> is one of the eight Esrum
compute nodes.</p>
<p>You can now run interactive jobs, for example running an R shell, or
test computationally expensive tools or scripts. However, note that you
<em>cannot</em> start jobs using Slurm in an interactive shell; jobs can only
be started from the head node.</p>
<p>Once you are done, be sure to exit the interactive shell by using the
<code class="docutils literal notranslate"><span class="pre">exit</span></code> command or pressing <code class="docutils literal notranslate"><span class="pre">Ctrl+D</span></code>, so that the resources reserved
for your shell is made available to other users!</p>
</div>
<div class="section" id="reserving-resources-for-your-jobs">
<span id="reserving-resources"></span><h2><span class="section-number">4.4.4. </span>Reserving resources for your jobs<a class="headerlink" href="#reserving-resources-for-your-jobs" title="Permalink to this headline">¶</a></h2>
<p>By default a <code class="docutils literal notranslate"><span class="pre">srun</span></code> will reserve 1 CPU and just under 15 GB of ram per
CPU. Should your job require more CPUs, then you can request them using
the <code class="docutils literal notranslate"><span class="pre">-c</span></code> or <code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code> options. The following runs a task
with 8 CPUs, and is automatically assigned 8 * 15 ~= 120 gigabytes of
RAM:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun -c <span class="m">8</span> -- my-command --threads <span class="m">8</span>
</pre></div>
</div>
<p>The amount of RAM allocated by default should be sufficient for most
tasks, but when needed you can request additional RAM using the
<code class="docutils literal notranslate"><span class="pre">--mem</span></code> or <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu</span></code> options. The following runs a task with 8
CPUs and 512 gigabytes of RAM:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun -c <span class="m">8</span> --mem 512G -- my-command --threads <span class="m">8</span>
</pre></div>
</div>
<p>As described in the <a class="reference internal" href="../overview.html#page-overview"><span class="std std-ref">Overview</span></a>, each node has 128 CPUs
available and 2 TB of RAM, of which 1993 GB can be reserved by Slurm.</p>
<p>The GPU node has 4 TB of RAM available, of which 3920 GB can be reserved
by Slurm, and may be used for jobs that have very high memory
requirements. However, since we only have one GPU node we ask that you
use the regular nodes unless your jobs actually require that much RAM.
See the next section for how to use the GPU node with or without
reserving a GPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that reserving CPUs only makes them available to your jobs,
it does not automatically make use of them! Check the documentation
for the software you are using to determine how to tell the software
to use additional threads (corresponding to the <code class="docutils literal notranslate"><span class="pre">--threads</span> <span class="pre">8</span></code>
arguments in the above example).</p>
</div>
<div class="section" id="best-practice-for-reserving-resources">
<h3><span class="section-number">4.4.4.1. </span>Best practice for reserving resources<a class="headerlink" href="#best-practice-for-reserving-resources" title="Permalink to this headline">¶</a></h3>
<p>Few programs benefit from using a lot of threads (CPUs) used due to
added overhead and due to limits to how much of a given process can be
parallelized. Maximum throughput is typically also limited by how fast
the software can read/write data. In some cases too many threads can
even increase the amount of time it takes to run the software, sometimes
drastically so!</p>
<p>We therefore recommended that you</p>
<blockquote>
<div><ul class="simple">
<li><p>Always refer to the documentation and recommendations for the
specific software you are using!</p></li>
<li><p>Test the effect of the number of threads you are using before
starting a lot of jobs.</p></li>
<li><p>Start with fewer CPUs and increase it only when there is a benefit
to doing so. You can for example start with 2, 4, or 8 CPUs per
task, and only increasing the number after it has been determined
that the software benefits from it.</p></li>
</ul>
</div></blockquote>
<p>Determining how many threads are necessary can be difficult, but the
<code class="docutils literal notranslate"><span class="pre">/usr/bin/time</span> <span class="pre">-f</span> <span class="pre">&quot;CPU</span> <span class="pre">=</span> <span class="pre">%P&quot;</span></code> command can be used to estimate the
efficiency from using multiple threads:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> /usr/bin/time -f <span class="s2">&quot;CPU = %P&quot;</span> my-command --threads <span class="m">1</span> ...
<span class="go">CPU = 99%</span>
<span class="gp">$</span> /usr/bin/time -f <span class="s2">&quot;CPU = %P&quot;</span> my-command --threads <span class="m">4</span> ...
<span class="go">CPU = 345%</span>
</pre></div>
</div>
<p>In this example increasing the number of threads/CPUs to 4 did not
result in a 4x increase in CPU usage, but only a 3.5x increase. And this
difference tends to increase the more threads are used.</p>
<p>A consequence of this is that it is often more efficient to split your
job into multiple sub-jobs (for example one job per chromosome) than
increasing the number of threads used for the individual jobs. See the
<a class="reference internal" href="batch_jobs.html#page-batch-jobs"><span class="std std-ref">Running batch jobs</span></a> page for more information.</p>
</div>
<div class="section" id="reserving-the-gpu-node">
<h3><span class="section-number">4.4.4.2. </span>Reserving the GPU node<a class="headerlink" href="#reserving-the-gpu-node" title="Permalink to this headline">¶</a></h3>
<p>This section describes how to schedule a task on the GPU node. The GPU
node is intended for tasks that need to use GPUs and for tasks that have
very high memory requirements (more than 2 TB).</p>
<p>To schedule a task on the GPU node you need to select the GPU queue and
(optionally) specify the number of Nvidia A100 GPUs needed (1 or 2). For
example, the following command queues the command <code class="docutils literal notranslate"><span class="pre">my-gpu-command</span></code> and
requests a single A100 GPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --gres<span class="o">=</span>gpu:a100:1 -- my-gpu-command
</pre></div>
</div>
<p>Alternatively you may reserve both GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --gres<span class="o">=</span>gpu:a100:2 -- my-gpu-command
</pre></div>
</div>
<p>If you on not need to use a GPU, then you can omit the <code class="docutils literal notranslate"><span class="pre">--gres</span></code>
option:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue -- my-command
</pre></div>
</div>
<p>As above you must also specify your CPU and RAM requirements using
<code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code> and <code class="docutils literal notranslate"><span class="pre">--mem</span></code>.</p>
</div>
<div class="section" id="monitoring-gpu-utilization">
<h3><span class="section-number">4.4.4.3. </span>Monitoring GPU utilization<a class="headerlink" href="#monitoring-gpu-utilization" title="Permalink to this headline">¶</a></h3>
<p>Slurm does not provide any means of monitoring the actual GPU
utilization, but tools such as <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> can be used to monitor
performance metrics. And since we are not going to actually <em>use</em> the
GPU, we can simply omit the <code class="docutils literal notranslate"><span class="pre">--gres</span></code> option.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you need to make use of GPU resources (passive monitoring
excluded), then you <em>must</em> also specify the appropriate <code class="docutils literal notranslate"><span class="pre">--gres</span></code>
option. Failure to do so will result in your jobs being terminated!</p>
</div>
<p>This allows slurm to run the monitoring task even when the GPUs are
reserved:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue -- nvidia-smi -l <span class="m">5</span>
<span class="go">Thu Jun  8 12:18:15 2023</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  NVIDIA A100 80G...  On   | 00000000:27:00.0 Off |                    0 |</span>
<span class="go">| N/A   43C    P0    47W / 300W |      0MiB / 81920MiB |      0%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>
<span class="go">|   1  NVIDIA A100 80G...  On   | 00000000:A3:00.0 Off |                    0 |</span>
<span class="go">| N/A   43C    P0    45W / 300W |      0MiB / 81920MiB |      0%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|  No running processes found                                                 |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">gpustat</span></code> tool provides a more convenient overview but must be
installed via <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> pip install gpustat
<span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --pty -- gpustat -i <span class="m">5</span>
<span class="go">esrumgpun01fl.unicph.domain  Thu Jun  8 12:20:24 2023  525.60.13</span>
<span class="go">[0] NVIDIA A100 80GB PCIe | 43°C,   0 % |     0 / 81920 MB |</span>
<span class="go">[1] NVIDIA A100 80GB PCIe | 43°C,   0 % |     0 / 81920 MB |</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">--pty</span></code> option is used in order to support colored, full-screen
output despite not running an interactive actual shell. As an
alternative, you can also start an interactive shell on the GPU node and
run <code class="docutils literal notranslate"><span class="pre">gpustats</span></code> or <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> that way:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --pty -- /bin/bash
<span class="gp">$</span> gpustat -i <span class="m">5</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="troubleshooting">
<h2><span class="section-number">4.4.5. </span>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h2>
<div class="section" id="error-requested-node-configuration-is-not-available">
<h3><span class="section-number">4.4.5.1. </span>Error: Requested node configuration is not available<a class="headerlink" href="#error-requested-node-configuration-is-not-available" title="Permalink to this headline">¶</a></h3>
<p>If you request too many CPUs (more than 128), or too much RAM (more than
1993 GB for compute nodes and more than 3920 GB for the GPU node), then
Slurm will report that the request cannot be satisfied:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># More than 128 CPUs requested</span>
$ srun --cpus-per-task <span class="m">200</span> -- <span class="nb">echo</span> <span class="s2">&quot;Hello world!&quot;</span>
srun: error: CPU count per node can not be satisfied
srun: error: Unable to allocate resources: Requested node configuration is not available

<span class="c1"># More than 1993 GB RAM requested on compute node</span>
$ srun --mem 2000G -- <span class="nb">echo</span> <span class="s2">&quot;Hello world!&quot;</span>
srun: error: Memory specification can not be satisfied
srun: error: Unable to allocate resources: Requested node configuration is not available
</pre></div>
</div>
<p>To solve this, simply reduce the number of CPUs and/or the amount of RAM
requested to fit within the limits described above. If your task does
require more than 1993 GB of RAM, then you also need to add the
<code class="docutils literal notranslate"><span class="pre">--partition=gpuqueue</span></code>, so that your task gets scheduled on the GPU
node.</p>
<p>Additionally, you may receive this message if you request GPUs without
specifying the correct queue or if you request too many GPUs:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># --partition=gpuqueue not specified</span>
$ srun --gres<span class="o">=</span>gpu:a100:2 -- <span class="nb">echo</span> <span class="s2">&quot;Hello world!&quot;</span>
srun: error: Unable to allocate resources: Requested node configuration is not available

<span class="c1"># More than 2 GPUs requested</span>
$ srun --partition<span class="o">=</span>gpuqueue --gres<span class="o">=</span>gpu:a100:3 -- <span class="nb">echo</span> <span class="s2">&quot;Hello world!&quot;</span>
srun: error: Unable to allocate resources: Requested node configuration is not available
</pre></div>
</div>
<p>To solve this error, simply avoid requesting more than 2 GPUs, and
remember to include the <code class="docutils literal notranslate"><span class="pre">--partition=gpuqueue</span></code> option.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h2><span class="section-number">4.4.6. </span>Additional resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Slurm <a class="reference external" href="https://slurm.schedmd.com/overview.html">documentation</a></p></li>
<li><p>Slurm <a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">summary</a> (PDF)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> <a class="reference external" href="https://slurm.schedmd.com/srun.html">manual page</a></p></li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="batch_jobs.html" title="4.5. Running batch jobs"
             >next</a></li>
        <li class="right" >
          <a href="filesystem.html" title="4.3. Projects, data, and home folders"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Esrum Cluster  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" ><span class="section-number">4. </span>Using the cluster</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, CBMR Phenomics.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>