

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Running jobs using Slurm &#8212; Esrum Cluster  documentation</title>
    <link rel="stylesheet" href="../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/playback.css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/js/libgif.js"></script>
    <script src="../_static/bizstyle.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Running batch jobs" href="batch_jobs.html" />
    <link rel="prev" title="The filesystem" href="filesystem.html" />
    <script defer src="../_static/js/playback.js"></script>

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="batch_jobs.html" title="Running batch jobs"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="filesystem.html" title="The filesystem"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Esrum Cluster  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Using the cluster</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Running jobs using Slurm</a><ul>
<li><a class="reference internal" href="#running-commands-using-slurm">Running commands using Slurm</a></li>
<li><a class="reference internal" href="#running-an-interactive-shell">Running an interactive shell</a></li>
<li><a class="reference internal" href="#reserving-resources-for-your-jobs">Reserving resources for your jobs</a><ul>
<li><a class="reference internal" href="#reserving-the-gpu-node">Reserving the GPU node</a></li>
</ul>
</li>
<li><a class="reference internal" href="#additional-resources">Additional resources</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="filesystem.html"
                        title="previous chapter">The filesystem</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="batch_jobs.html"
                        title="next chapter">Running batch jobs</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="running-jobs-using-slurm">
<span id="page-running"></span><h1>Running jobs using Slurm<a class="headerlink" href="#running-jobs-using-slurm" title="Permalink to this headline">¶</a></h1>
<p>In order to run jobs on the Esrum cluster, you must connect to the head
node and queue them using the <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a> job management system. <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a> takes
care of automatically queuing and distribute jobs on the compute and GPU
nodes when the required resources are available.</p>
<p>This section describes how to run basic jobs, how to start an
interactive shell on a compute node, and how to reserve the resources
needed for your tasks.</p>
<p>If you need to run a number of similar jobs in parallel, for example
genotyping a set of samples or mapping FASTQ files to a reference
genome, then the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> can be used to automatically queue multiple
jobs. See the <a class="reference internal" href="batch_jobs.html#page-batch-jobs"><span class="std std-ref">Running batch jobs</span></a> for more information.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Resource intensive jobs <em>must</em> be run using <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a>. Tasks running on
the head node <em>will</em> be terminated without prior warning, in order to
prevent any impact on other users of the cluster.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Show consideration towards other users of the cluster. If you need to
run very intensive jobs then please <a class="reference internal" href="../contact.html#page-contact"><span class="std std-ref">Contact</span></a> Phenomics
first so that we can help ensure that the cluster will still be
usable by other users while your jobs are running. Failure to do so
may result in your jobs being terminated without prior warning.</p>
</div>
<div class="section" id="running-commands-using-slurm">
<h2>Running commands using Slurm<a class="headerlink" href="#running-commands-using-slurm" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> command is used to queue and execute commands on the
compute nodes, and for most part it should feel no different than
running a command without <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm</a>. Simply prefix your command with
<code class="docutils literal notranslate"><span class="pre">srun</span></code> and the queuing system takes care of running it on the first
available compute node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun gzip chr20.fasta
</pre></div>
</div>
<img alt="../_images/srun_minimal.gif" class="gif" src="../_images/srun_minimal.gif" />
<p>Except for the <code class="docutils literal notranslate"><span class="pre">srun</span></code> prefix, this is exactly as if you ran the
<code class="docutils literal notranslate"><span class="pre">gzip</span></code> command on the head node. However, if you need to pipe output
to a file or to another command, then you <em>must</em> wrap your commands in a
bash (or similar) script. The script can then be run using <code class="docutils literal notranslate"><span class="pre">srun</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun bash my_script.sh
</pre></div>
</div>
<img alt="../_images/srun_wrapped.gif" class="gif" src="../_images/srun_wrapped.gif" />
<p>For tips to make your bash scripts more robust, see the <a class="reference internal" href="../tips_and_tricks/safer_bash_scripts.html#page-bash"><span class="std std-ref">Writing reliable bash scripts</span></a>
page.</p>
<p>By default task are allocated one CPU and 15 GB of RAM. If you need to
use additional resources, then see <a class="reference internal" href="#reserving-resources-for-your-jobs">Reserving resources for your jobs</a>
and <a class="reference internal" href="#reserving-the-gpu-node">Reserving the GPU node</a> below.</p>
</div>
<div class="section" id="running-an-interactive-shell">
<h2>Running an interactive shell<a class="headerlink" href="#running-an-interactive-shell" title="Permalink to this headline">¶</a></h2>
<p>If you need to run an interactive process, for example if you need to
use an interactive R shell to process a large dataset, or if you just
need to experiment with running an computationally heavy process, then
you can start a shell on one of the compute nodes as follows:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">[abc123@esrumhead01fl ~] $</span> srun --pty -- /bin/bash
<span class="gp">[abc123@esrumcmpn07fl ~] $</span>
</pre></div>
</div>
<p>Note that the hostname displayed changes from <code class="docutils literal notranslate"><span class="pre">esrumhead01fl</span></code> to
<code class="docutils literal notranslate"><span class="pre">esrumcmpn07fl</span></code>, where <code class="docutils literal notranslate"><span class="pre">esrumcmpn07fl</span></code> is one of the eight Esrum
compute nodes.</p>
<p>You can now run interactive jobs, for example running an R shell, or
test computationally expensive tools or scripts. However, note that you
<em>cannot</em> start jobs using Slurm in an interactive shell; jobs can only
be started from the head node.</p>
<p>Once you are done, be sure to exit the interactive shell by using the
<code class="docutils literal notranslate"><span class="pre">exit</span></code> command or pressing <code class="docutils literal notranslate"><span class="pre">Ctrl+D</span></code>, so that the resources reserved
for your shell is made available to other users!</p>
</div>
<div class="section" id="reserving-resources-for-your-jobs">
<h2>Reserving resources for your jobs<a class="headerlink" href="#reserving-resources-for-your-jobs" title="Permalink to this headline">¶</a></h2>
<p>By default a <code class="docutils literal notranslate"><span class="pre">srun</span></code> will reserve 1 CPU and just under 15 GB of ram per
CPU. Should your job require more CPUs, then you can request them using
the <code class="docutils literal notranslate"><span class="pre">-c</span></code> or <code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code> options. The following runs a task
with 8 CPUs and 8 * 15 = 120 gigabytes of RAM:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun -c <span class="m">8</span> -- my-command
</pre></div>
</div>
<p>The amount of RAM allocated by default should be sufficient for most
tasks, but when needed you can request additional RAM using the
<code class="docutils literal notranslate"><span class="pre">--mem</span></code> or <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu</span></code> options. The following runs a task with 8
CPUs and 512 gigabytes of RAM:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun -c <span class="m">8</span> --mem 512G -- my-command
</pre></div>
</div>
<p>As described in the <a class="reference internal" href="../overview.html#page-overview"><span class="std std-ref">Overview</span></a>, each node has 128 CPUs
available and 2048 GB of RAM, of which about 1993 GB can be reserved in
total.</p>
<p>The GPU node has 4096 GB of RAM, of which 3920 is reservable, but since
we only have one GPU node we ask that you use the regular nodes unless
your analyses actually requires that much RAM at once. See the next
section for how to use the GPU node with or without reserving a GPU.</p>
<div class="section" id="reserving-the-gpu-node">
<h3>Reserving the GPU node<a class="headerlink" href="#reserving-the-gpu-node" title="Permalink to this headline">¶</a></h3>
<p>This section describes how to schedule a task on the GPU node. The GPU
node is intended for tasks that need to use GPUs and for tasks that have
very high memory requirements (more than 2 TB).</p>
<p>To schedule a task on the GPU node you need to select the GPU queue and
(optionally) specify the number of Nvidia A100 GPUs (1 or 2) needed. The
following command queues command <code class="docutils literal notranslate"><span class="pre">my-gpu-command</span></code> and requests a
single A100 GPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --gres<span class="o">=</span>gpu:a100:1 -- my-gpu-command
</pre></div>
</div>
<p>Alternatively you may reserve both CPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --gres<span class="o">=</span>gpu:a100:2 -- my-gpu-command
</pre></div>
</div>
<p>If you on not need to use a GPU, then you can omit the <cite>–gres</cite> option:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue -- my-command
</pre></div>
</div>
<p>As above you must also specify your CPU and RAM requirements using</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you need to make use of GPU resources (monitoring excluded), then
you <em>must</em> also specify the appropriate <cite>–gres</cite> option. Failure to
do so will result in your jobs being terminated!</p>
</div>
<p>Slurm does not provide any means of monitoring the actual GPU usage, but
tool such as <cite>nvidia-smi</cite> can be used to monitor performance metrics.
Since we are not going to actually <em>use</em> the GPU, we can simply omit the
<cite>–gres</cite> option. This allows slurm to run the task even when the GPUs
are reserved.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue -- nvidia-smi -l <span class="m">5</span>
<span class="go">Thu Jun  8 12:18:15 2023</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  NVIDIA A100 80G...  On   | 00000000:27:00.0 Off |                    0 |</span>
<span class="go">| N/A   43C    P0    47W / 300W |      0MiB / 81920MiB |      0%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>
<span class="go">|   1  NVIDIA A100 80G...  On   | 00000000:A3:00.0 Off |                    0 |</span>
<span class="go">| N/A   43C    P0    45W / 300W |      0MiB / 81920MiB |      0%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|  No running processes found                                                 |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>The <cite>gpustat</cite> tool provides a more convenient overview but must be
installed via <cite>pip</cite>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> pip install gpustat
<span class="gp">$</span> srun --partition<span class="o">=</span>gpuqueue --pty -- gpustat -i <span class="m">5</span>
<span class="go">esrumgpun01fl.unicph.domain  Thu Jun  8 12:20:24 2023  525.60.13</span>
<span class="go">[0] NVIDIA A100 80GB PCIe | 43°C,   0 % |     0 / 81920 MB |</span>
<span class="go">[1] NVIDIA A100 80GB PCIe | 43°C,   0 % |     0 / 81920 MB |</span>
</pre></div>
</div>
<p>The <cite>–pty</cite> option is used in order to support colored, full-screen
output despite not running an interactive actual shell.</p>
<p>As an alternative, you can also start an interactive shell on the GPU
node:</p>
<blockquote>
<div><p>$ srun –partition=gpuqueue –pty – /bin/bash</p>
</div></blockquote>
</div>
</div>
<div class="section" id="additional-resources">
<h2>Additional resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> <a class="reference external" href="https://slurm.schedmd.com/srun.html">manual page</a></p></li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="batch_jobs.html" title="Running batch jobs"
             >next</a></li>
        <li class="right" >
          <a href="filesystem.html" title="The filesystem"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Esrum Cluster  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >Using the cluster</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, CBMR Phenomics.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>